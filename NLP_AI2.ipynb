{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Intro to NLP\n",
        "Look at the technologies around us:\n",
        "\n",
        "Spellcheck and autocorrect\n",
        "Auto-generated video captions\n",
        "Virtual assistants like Amazon’s Alexa\n",
        "Autocomplete\n",
        "Your news site’s suggested articles"
      ],
      "metadata": {
        "id": "zpIu4gu35QVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All of these handy technologies exist because of natural language processing! Also known as NLP, the field is at the intersection of linguistics, artificial intelligence, and computer science. The goal? Enabling computers to interpret, analyze, and approximate the generation of human languages (like English or Spanish).\n",
        "\n",
        "NLP got its start around 1950 with Alan Turing’s test for artificial intelligence evaluating whether a computer can use language to fool humans into believing it’s human.\n",
        "\n",
        "But approximating human speech is only one of a wide range of applications for NLP! Applications from detecting spam emails or bias in tweets to improving accessibility for people with disabilities all rely heavily on natural language processing techniques.\n",
        "\n",
        "NLP can be conducted in several programming languages. However, Python has some of the most extensive open-source NLP libraries, including the Natural Language Toolkit or NLTK. Because of this, you’ll be using Python to get your first taste of NLP."
      ],
      "metadata": {
        "id": "NwaDW5955bIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning and preparation are crucial for many tasks, and NLP is no exception. Text preprocessing is usually the first step you’ll take when faced with an NLP task.\n",
        "\n",
        "Without preprocessing, your computer interprets \"the\", \"The\", and \"<p>The\" as entirely different words. There is a LOT you can do here, depending on the formatting you need. Lucky for you, Regex and NLTK will do most of it for you! Common tasks include:\n",
        "\n",
        "Noise removal — stripping text of formatting (e.g., HTML tags).\n",
        "\n",
        "Tokenization — breaking text into individual words.\n",
        "\n",
        "Normalization — cleaning text data in any other way:\n",
        "\n",
        "Stemming is a blunt axe to chop off word prefixes and suffixes. “booing” and “booed” become “boo”, but “computer” may become “comput” and “are” would remain “are.”\n",
        "Lemmatization is a scalpel to bring words down to their root forms. For example, NLTK’s savvy lemmatizer knows “am” and “are” are related to “be.”"
      ],
      "metadata": {
        "id": "b7ckfgA35umU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is NLP?\n",
        "1)Natural language processing combines computer science, linguistics, and artificial intelligence to enable computers to process human languages.\n",
        "2)NLTK is a Python library used for NLP.\n",
        "3)Text preprocessing is a stage of NLP focused on cleaning and preparing text for other NLP tasks.\n",
        "Parsing is an NLP technique concerned with breaking up text based on syntax.\n"
      ],
      "metadata": {
        "id": "Kvw6V0iI3OpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4)Language models are probabilistic machine models of language use for NLP comprehension tasks. Common models include bag-of-words, n-gram models, and neural language modeling.\n",
        "5)Topic modeling is the NLP process by which hidden topics are identified given a body of text.\n",
        "6)Text similarity is a facet of NLP concerned with semblance between instances of language.\n",
        "7)Language prediction is an application of NLP concerned with predicting language given preceding language.\n",
        "8)There are many social and ethical considerations to take into account when designing NLP tools."
      ],
      "metadata": {
        "id": "oOOPUkmG30jG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Levenshtein distance:\n",
        "from nltk.metrics import edit_distance\n",
        "\n",
        "# an arbitrary plagiarism classifier:\n",
        "def is_plagiarized(text1, text2):\n",
        "  n = 7\n",
        "  if edit_distance(text1.lower(), text2.lower()) > ((len(text1) + len(text2)) / n):\n",
        "    return False\n",
        "  return True\n",
        "\n",
        "doc1 = \"is this plagiarized\"\n",
        "doc2 = \"it's not plagiarized\"\n",
        "\n",
        "print(is_plagiarized(doc1, doc2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1niydx-3aTR",
        "outputId": "e198e283-3794-43a8-80b3-e0596d3db546"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing regex and nltk\n",
        "import re, nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# importing Counter to get word counts for bag of words\n",
        "from collections import Counter\n",
        "# importing a passage from Through the Looking Glass\n",
        "from looking_glass import looking_glass_text\n",
        "# importing part-of-speech function for lemmatization\n",
        "from part_of_speech import get_part_of_speech\n",
        "\n",
        "# Change text to another string:\n",
        "text = looking_glass_text\n",
        "\n",
        "cleaned = re.sub('\\W+', ' ', text).lower()\n",
        "tokenized = word_tokenize(cleaned)\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "filtered = [word for word in tokenized if word not in stop_words]\n",
        "\n",
        "normalizer = WordNetLemmatizer()\n",
        "normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in filtered]\n",
        "# Comment out the print statement below\n",
        "print(normalized)\n",
        "\n",
        "# Define bag_of_looking_glass_words & print:\n",
        "bag_of_looking_glass_words= Counter(normalized)\n",
        "print(bag_of_looking_glass_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "EcjtwZtM-Fwz",
        "outputId": "7c662a40-af54-4f22-f2f2-ef1f08f60f41"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'looking_glass'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-f20ca65549da>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# importing a passage from Through the Looking Glass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlooking_glass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlooking_glass_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# importing part-of-speech function for lemmatization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpart_of_speech\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_part_of_speech\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'looking_glass'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing Text\n",
        "You now have a preprocessed, clean list of words. Now what? It may be helpful to know how the words relate to each other and the underlying syntax (grammar). Parsing is an NLP process concerned with segmenting text based on syntax."
      ],
      "metadata": {
        "id": "54p81yAX80H-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advanced NLP Topics\n",
        "There are a slew of advanced topics and applications of NLP, many of which rely on deep learning and neural networks.\n",
        "\n",
        "Naive Bayes classifiers are supervised machine learning algorithms that leverage a probabilistic theorem to make predictions and classifications. They are widely used for sentiment analysis (determining whether a given block of language expresses negative or positive feelings) and spam filtering.\n",
        "\n",
        "I made enormous gains in machine translation, but even the most advanced translation software using neural networks and LSTM still has far to go in accurately translating between languages.\n",
        "\n",
        "Some of the most life-altering applications of NLP are focused on improving language accessibility for people with disabilities. Text-to-speech functionality and speech recognition have improved rapidly thanks to neural language models, making digital spaces far more accessible places.\n",
        "\n",
        "NLP can also be used to detect bias in writing and speech. Feel like a political candidate, book, or news source is biased but can’t put your finger on exactly how? Natural language processing can help you identify the language at issue."
      ],
      "metadata": {
        "id": "Eh42ns9q4Xzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk, re\n",
        "from nltk.tokenize import word_tokenize\n",
        "# importing ngrams module from nltk\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "from looking_glass import looking_glass_full_text\n",
        "\n",
        "cleaned = re.sub('\\W+', ' ', looking_glass_full_text).lower()\n",
        "tokenized = word_tokenize(cleaned)\n",
        "\n",
        "# Change the n value to 2:\n",
        "looking_glass_bigrams = ngrams(tokenized, 2)\n",
        "looking_glass_bigrams_frequency = Counter(looking_glass_bigrams)\n",
        "\n",
        "# Change the n value to 3:\n",
        "looking_glass_trigrams = ngrams(tokenized, 3)\n",
        "looking_glass_trigrams_frequency = Counter(looking_glass_trigrams)\n",
        "\n",
        "# Change the n value to a number greater than 3:\n",
        "looking_glass_ngrams = ngrams(tokenized, 8)\n",
        "looking_glass_ngrams_frequency = Counter(looking_glass_ngrams)\n",
        "\n",
        "print(\"Looking Glass Bigrams:\")\n",
        "print(looking_glass_bigrams_frequency.most_common(10))\n",
        "\n",
        "print(\"\\nLooking Glass Trigrams:\")\n",
        "print(looking_glass_trigrams_frequency.most_common(10))\n",
        "\n",
        "print(\"\\nLooking Glass n-grams:\")\n",
        "print(looking_glass_ngrams_frequency.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "z7SDoXlV5DjH",
        "outputId": "e9ad6c10-e3f6-49d9-e2c6-47fa728f4949"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'looking_glass'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-01380a91f59b>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlooking_glass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlooking_glass_full_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\W+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlooking_glass_full_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'looking_glass'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}